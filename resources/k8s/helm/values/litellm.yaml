---
# Default values for litellm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
image:
  # Use "ghcr.io/berriai/litellm-database" for optimized image with database
    repository: ghcr.io/berriai/litellm-database
    pullPolicy: Always
  # Overrides the image tag whose default is the chart appVersion.
  # tag: "main-latest"
    tag: main-v1.73.0-stable
imagePullSecrets: []
nameOverride: litellm
fullnameOverride: ''
serviceAccount:
  # Specifies whether a service account should be created
    create: true
  # Automatically mount a ServiceAccount's API credentials?
    automount: true
  # Annotations to add to the service account
    annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
    name: litellm
podAnnotations: {}
podLabels: {}
# At the time of writing, the litellm docker image requires write access to the
#  filesystem on startup so that prisma can install some dependencies.
podSecurityContext: {}
securityContext: {}
# capabilities:
#   drop:
#     - ALL
# readOnlyRootFilesystem: false
# runAsNonRoot: true
# runAsUser: 1000
# A list of Kubernetes Secret objects that will be exported to the LiteLLM proxy
#  pod as environment variables.  These secrets can then be referenced in the
#  configuration file (or "litellm" ConfigMap) with `os.environ/<Env Var Name>`
environmentSecrets:
    - litellm-env-secret
# A list of Kubernetes ConfigMap objects that will be exported to the LiteLLM proxy
#  pod as environment variables.  The ConfigMap kv-pairs can then be referenced in the
#  configuration file (or "litellm" ConfigMap) with `os.environ/<Env Var Name>`
environmentConfigMaps:
    - litellm-env-configmap
service:
    type: ClusterIP
    port: 4000
ingress:
    enabled: true
    className: nginx
    annotations:
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/proxy-body-size: 50m
  # kubernetes.io/tls-acme: "true"
    hosts:
        - host: litellm.tqtensor.com
          paths:
              - path: /
                pathType: ImplementationSpecific
    tls:
        - secretName: litellm-tls-secret
          hosts:
              - litellm.tqtensor.com
masterkey: changeme
# The elements within proxy_config are rendered as config.yaml for the proxy
#  Examples: https://github.com/BerriAI/litellm/tree/main/litellm/proxy/example_config_yaml
#  Reference: https://docs.litellm.ai/docs/proxy/configs
proxy_config:
    credential_list:
        - credential_name: vertex_ai_key
          credential_values:
              vertex_credentials: os.environ/VERTEX_SA_KEY
              vertex_location: us-central1
          credential_info:
              description: Vertex AI primary service account
        - credential_name: vertex_ai_2nd_key
          credential_values:
              vertex_credentials: os.environ/VERTEX_SA_2ND_KEY
              vertex_location: us-central1
          credential_info:
              description: Vertex AI secondary service account
        - credential_name: bedrock
          credential_values:
              aws_access_key_id: os.environ/BEDROCK_AWS_ACCESS_KEY_ID
              aws_secret_access_key: os.environ/BEDROCK_AWS_SECRET_ACCESS_KEY
              aws_region_name: us-east-1
          credential_info:
              description: Bedrock AWS credentials (us-east-1)
        - credential_name: bedrock_west
          credential_values:
              aws_access_key_id: os.environ/BEDROCK_AWS_ACCESS_KEY_ID
              aws_secret_access_key: os.environ/BEDROCK_AWS_SECRET_ACCESS_KEY
              aws_region_name: us-west-2
          credential_info:
              description: Bedrock AWS credentials (us-west-2)
        - credential_name: azure
          credential_values:
              api_base: os.environ/AZURE_API_BASE
              api_key: os.environ/AZURE_API_KEY
              api_version: 2024-12-01-preview
          credential_info:
              description: Azure OpenAI credentials
        - credential_name: openrouter
          credential_values:
              api_base: https://openrouter.ai/api/v1
              api_key: os.environ/OPENROUTER_API_KEY
          credential_info:
              description: OpenRouter API credentials
        - credential_name: vllm
          credential_values:
              api_base: https://vllm.tqtensor.com/v1
              api_key: os.environ/VLLM_API_KEY
          credential_info:
              description: VLLM API credentials
        - credential_name: groq
          credential_values:
              api_key: os.environ/GROQ_API_KEY
          credential_info:
              description: Groq API credentials
    model_list:
        - model_name: google-gemini-flash-2.0
          litellm_params:
              model: vertex_ai/gemini-2.0-flash-001
              litellm_credential_name: vertex_ai_key
        - model_name: google-gemini-flash-2.0
          litellm_params:
              model: vertex_ai/gemini-2.0-flash-001
              litellm_credential_name: vertex_ai_2nd_key
        - model_name: google-gemini-flash-2.5
          litellm_params:
              model: vertex_ai/gemini-2.5-flash
              litellm_credential_name: vertex_ai_key
        - model_name: google-gemini-flash-2.5
          litellm_params:
              model: vertex_ai/gemini-2.5-flash
              litellm_credential_name: vertex_ai_2nd_key
        - model_name: google-gemini-pro-2.5
          litellm_params:
              model: vertex_ai/gemini-2.5-pro
              litellm_credential_name: vertex_ai_key
        - model_name: google-gemini-pro-2.5
          litellm_params:
              model: vertex_ai/gemini-2.5-pro
              litellm_credential_name: vertex_ai_2nd_key
        - model_name: google-multilingual-embedding
          litellm_params:
              model: vertex_ai/text-multilingual-embedding-002
              litellm_credential_name: vertex_ai_key
        - model_name: google-multilingual-embedding
          litellm_params:
              model: vertex_ai/text-multilingual-embedding-002
              litellm_credential_name: vertex_ai_2nd_key
        - model_name: google-text-embedding-large
          litellm_params:
              model: vertex_ai/text-embedding-large-exp-03-07
              litellm_credential_name: vertex_ai_key
        - model_name: google-text-embedding-large
          litellm_params:
              model: vertex_ai/text-embedding-large-exp-03-07
              litellm_credential_name: vertex_ai_2nd_key
        - model_name: google-gemini-embedding-001
          litellm_params:
              model: vertex_ai/gemini-embedding-001
              litellm_credential_name: vertex_ai_key
        - model_name: google-gemini-embedding-001
          litellm_params:
              model: vertex_ai/gemini-embedding-001
              litellm_credential_name: vertex_ai_2nd_key
        - model_name: bedrock-claude-3.7-sonnet
          litellm_params:
              model: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0
              litellm_credential_name: bedrock
        - model_name: bedrock-claude-4.0-sonnet
          litellm_params:
              model: bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0
              litellm_credential_name: bedrock_west
        - model_name: azure-gpt-41
          litellm_params:
              model: azure/gpt-4.1
              litellm_credential_name: azure
        - model_name: azure-gpt-45
          litellm_params:
              model: azure/gpt-4.5
              litellm_credential_name: azure
        - model_name: azure-o4-mini
          litellm_params:
              model: azure/o4-mini
              litellm_credential_name: azure
        - model_name: openrouter-anthropic-claude-opus-4
          litellm_params:
              model: openrouter/anthropic/claude-opus-4
              litellm_credential_name: openrouter
        - model_name: openrouter-anthropic-claude-sonnet-4
          litellm_params:
              model: openrouter/anthropic/claude-sonnet-4
              litellm_credential_name: openrouter
        - model_name: vllm-menlo-jan-nano
          litellm_params:
              model: openai/Menlo/Jan-nano
              litellm_credential_name: vllm
        - model_name: groq-stx
          litellm_params:
              model: groq/meta-llama/llama-4-maverick-17b-128e-instruct
              litellm_credential_name: groq
        - model_name: groq-stx
          litellm_params:
              model: groq/meta-llama/llama-4-scout-17b-16e-instruct
              litellm_credential_name: groq
        - model_name: groq-stx
          litellm_params:
              model: groq/mistral-saba-24b
              litellm_credential_name: groq
        - model_name: groq-stx
          litellm_params:
              model: groq/qwen/qwen3-32b
              litellm_credential_name: groq
    general_settings:
        master_key: os.environ/PROXY_MASTER_KEY
        store_model_in_db: true
        store_prompts_in_spend_logs: true
    litellm_settings:
        drop_params: true
        success_callback: [langfuse]
    router_settings:
        routing_strategy: simple-shuffle
        num_retries: 2
        timeout: 30
        fallbacks: [groq-stx: [azure-gpt-41], google-gemini-flash-2.5: [azure-gpt-41], google-gemini-pro-2.5: [azure-gpt-41]]
resources: {}
# We usually recommend not to specify default resources and to leave this as a conscious
# choice for the user. This also increases chances charts run on environments with little
# resources, such as Minikube. If you do want to specify resources, uncomment the following
# lines, adjust them as necessary, and remove the curly braces after 'resources:'.
# limits:
#   cpu: 100m
#   memory: 128Mi
# requests:
#   cpu: 100m
#   memory: 128Mi
autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80
# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false
# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true
nodeSelector: {}
tolerations: []
affinity: {}
db:
  # Use an existing postgres server/cluster
    useExisting: true
  # How to connect to the existing postgres server/cluster
    endpoint: localhost
    database: litellm
    url: postgresql://$(DATABASE_USERNAME):$(DATABASE_PASSWORD)@$(DATABASE_HOST)/$(DATABASE_NAME)
    secret:
        name: litellm-postgres-secret
        usernameKey: username
        passwordKey: password
  # Use the Stackgres Helm chart to deploy an instance of a Stackgres cluster.
  #  The Stackgres Operator must already be installed within the target
  #  Kubernetes cluster.
  # TODO: Stackgres deployment currently unsupported
    useStackgresOperator: false
  # Use the Postgres Helm chart to create a single node, stand alone postgres
  #  instance.  See the "postgresql" top level key for additional configuration.
    deployStandalone: false
# Settings for Bitnami postgresql chart (if db.deployStandalone is true, ignored
#  otherwise)
postgresql:
    architecture: standalone
    auth:
        username: litellm
        database: litellm
    # You should override these on the helm command line with
    #  `--set postgresql.auth.postgres-password=<some good password>,postgresql.auth.password=<some good password>`
        password: HQJGdZgigJe05Mmt
        postgres-password: HQJGdZgigJe05Mmt
  # A secret is created by this chart (litellm-helm) with the credentials that
  #  the new Postgres instance should use.
  # existingSecret: ""
  # secretKeys:
  #   userPasswordKey: password
# requires cache: true in config file
# either enable this or pass a secret for REDIS_HOST, REDIS_PORT, REDIS_PASSWORD or REDIS_URL
# with cache: true to use existing redis instance
redis:
    enabled: true
    architecture: standalone
# Prisma migration job settings
migrationJob:
  # Enable or disable the schema migration Job
    enabled: true
  # Number of retries for the Job in case of failure
    retries: 3
  # Backoff limit for Job restarts
    backoffLimit: 4
  # Skip schema migrations for specific environments. When True, the job will exit with code 0.
    disableSchemaUpdate: false
    annotations: {}
# Additional environment variables to be added to the deployment
envVars: {}
# USE_DDTRACE: "true"
